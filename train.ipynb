{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from nibabel.viewers import OrthoSlicer3D\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "\n",
    "import nibabel as nib\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import dice\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import json\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data\n",
    "would takes 1.5 min on PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './data/RawData/Training/img/'\n",
    "img_filenames = sorted(os.listdir(img_path))\n",
    "label_path = './data/RawData/Training/label/'\n",
    "label_filenames = sorted(os.listdir(label_path))\n",
    "\n",
    "bbox_coords = {}\n",
    "ground_truth_mask = {}\n",
    "data_count = 0\n",
    "for img_num in range(0, 30):\n",
    "    # read image and label\n",
    "    image_nib = nib.load(img_path + img_filenames[img_num])\n",
    "    image = image_nib.get_fdata().astype(np.uint8)\n",
    "    x, y, z = image.shape\n",
    "    label_nib = nib.load(label_path + label_filenames[img_num])\n",
    "    label = label_nib.get_fdata().astype(np.uint8)\n",
    "    \n",
    "    for h in range(z):\n",
    "        image0 = image[:, :, h]\n",
    "        label0 = label[:, :, h]\n",
    "        \n",
    "        for i in range(1, 14):\n",
    "            org_label = np.where(label0 == i, 1, 0)\n",
    "            nonzero_indices = np.argwhere(org_label == 1)\n",
    "            if len(nonzero_indices) < 10:   # if the number of non-zero indices is too small, skip\n",
    "                continue\n",
    "            x_min = np.min(nonzero_indices[:, 0])\n",
    "            y_min = np.min(nonzero_indices[:, 1])\n",
    "            x_max = np.max(nonzero_indices[:, 0])\n",
    "            y_max = np.max(nonzero_indices[:, 1])\n",
    "                \n",
    "            bbox = np.array([[x_min-3, y_min-3, x_max+3, y_max+3]])\n",
    "            \n",
    "            bbox_coords[data_count] = bbox.tolist()\n",
    "            ground_truth_mask[data_count] = org_label.tolist()\n",
    "            data_count += 1\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sam_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_model = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam_model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the input images into a format SAM's internal functions expect.\n",
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "transformed_data = defaultdict(dict)\n",
    "for k in bbox_coords.keys():\n",
    "  image = cv2.imread(f'scans/scans/{k}.png')\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "  input_image = transform.apply_image(image)\n",
    "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "  \n",
    "  input_image = sam_model.preprocess(transformed_image)\n",
    "  original_image_size = image.shape[:2]\n",
    "  input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "  transformed_data[k]['image'] = input_image\n",
    "  transformed_data[k]['input_size'] = input_size\n",
    "  transformed_data[k]['original_image_size'] = original_image_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
