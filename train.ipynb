{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from nibabel.viewers import OrthoSlicer3D\n",
    "\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "\n",
    "import nibabel as nib\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.functional import dice\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import json\n",
    "\n",
    "from segment_anything import SamPredictor, sam_model_registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data\n",
    "would takes 1.5 min on PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[110, 167, 202, 253]]), 1: array([[312, 137, 395, 215]]), 2: array([[165, 287, 187, 312]]), 3: array([[ 59, 190, 316, 388]]), 4: array([[242, 300, 382, 377]]), 5: array([[249, 242, 282, 274]]), 6: array([[193, 243, 252, 270]]), 7: array([[216, 265, 291, 327]])}\n"
     ]
    }
   ],
   "source": [
    "img_path = './data/RawData/Training/img/'\n",
    "img_filenames = sorted(os.listdir(img_path))\n",
    "label_path = './data/RawData/Training/label/'\n",
    "label_filenames = sorted(os.listdir(label_path))\n",
    "\n",
    "bbox_coords = {}\n",
    "ground_truth_mask = {}\n",
    "images = {}\n",
    "data_count = 0\n",
    "'''\n",
    "for img_num in range(0, 30):\n",
    "    # read image and label\n",
    "    image_nib = nib.load(img_path + img_filenames[img_num])\n",
    "    image = image_nib.get_fdata().astype(np.uint8)\n",
    "    x, y, z = image.shape\n",
    "    label_nib = nib.load(label_path + label_filenames[img_num])\n",
    "    label = label_nib.get_fdata().astype(np.uint8)\n",
    "    \n",
    "    for h in range(z):\n",
    "        image0 = image[:, :, h]\n",
    "        label0 = label[:, :, h]\n",
    "        \n",
    "        for i in range(1, 14):\n",
    "            org_label = np.where(label0 == i, 1, 0)\n",
    "            nonzero_indices = np.argwhere(org_label == 1)\n",
    "            if len(nonzero_indices) < 10:   # if the number of non-zero indices is too small, skip\n",
    "                continue\n",
    "            x_min = np.min(nonzero_indices[:, 0])\n",
    "            y_min = np.min(nonzero_indices[:, 1])\n",
    "            x_max = np.max(nonzero_indices[:, 0])\n",
    "            y_max = np.max(nonzero_indices[:, 1])\n",
    "                \n",
    "            bbox = np.array([[x_min-3, y_min-3, x_max+3, y_max+3]])\n",
    "            \n",
    "            bbox_coords[data_count] = bbox.tolist()\n",
    "            ground_truth_mask[data_count] = org_label.tolist()\n",
    "            data_count += 1\n",
    "'''\n",
    "image_nib = nib.load(img_path + img_filenames[0])\n",
    "image = image_nib.get_fdata().astype(np.uint8)\n",
    "x, y, z = image.shape\n",
    "label_nib = nib.load(label_path + label_filenames[0])\n",
    "label = label_nib.get_fdata().astype(np.uint8)\n",
    "image0, label0 = image[:, :, 100], label[:, :, 100]\n",
    "for i in range(1, 14):\n",
    "    org_label = np.where(label0 == i, 1, 0)\n",
    "    nonzero_indices = np.argwhere(org_label == 1)\n",
    "    if len(nonzero_indices) < 10:   # if the number of non-zero indices is too small, skip\n",
    "        continue\n",
    "    x_min = np.min(nonzero_indices[:, 0])\n",
    "    y_min = np.min(nonzero_indices[:, 1])\n",
    "    x_max = np.max(nonzero_indices[:, 0])\n",
    "    y_max = np.max(nonzero_indices[:, 1])\n",
    "                \n",
    "    bbox = np.array([[x_min-3, y_min-3, x_max+3, y_max+3]])\n",
    "            \n",
    "    bbox_coords[data_count] = bbox\n",
    "    ground_truth_mask[data_count] = org_label\n",
    "    images[data_count] = image0\n",
    "    data_count += 1\n",
    "\n",
    "print(bbox_coords)            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_checkpoint = \"./checkpoints/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_model = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam_model.to(device=device)\n",
    "sam_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the input images into a format SAM's internal functions expect.\n",
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "transformed_data = defaultdict(dict)\n",
    "for k in bbox_coords.keys():\n",
    "  image = images[k]\n",
    "  image = np.stack([image, image, image], axis=-1)\n",
    "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "  input_image = transform.apply_image(image)\n",
    "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "  \n",
    "  input_image = sam_model.preprocess(transformed_image)\n",
    "  original_image_size = image.shape[:2]\n",
    "  input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "  transformed_data[k]['image'] = input_image\n",
    "  transformed_data[k]['input_size'] = input_size\n",
    "  transformed_data[k]['original_image_size'] = original_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "loss_fn1 = torch.nn.MSELoss()\n",
    "# loss_fn1 = torch.nn.BCELoss()\n",
    "def loss_fn(pred, target):\n",
    "  return 1 - dice(pred, target)\n",
    "keys = list(bbox_coords.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Fine Tuning\n",
    "\n",
    "This is the main training loop.\n",
    "\n",
    "Improvements to be made include batching and moving the computation of the image and prompt embeddings outside the loop since we are not tuning these parts of the model, this will speed up training as we should not recompute the embeddings during each epoch. Sometimes the optimizer gets lost in the parameter space and the loss function blows up. Restarting from scratch (including running all cells below 'Prepare Fine Tuning' in order to start with default weights again) will solve this.\n",
    "\n",
    "In a production implementation a better choice of optimiser/loss function will certainly help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Mean loss: 0.02435454719234258\n",
      "EPOCH: 1\n",
      "Mean loss: 0.02435467003379017\n",
      "EPOCH: 2\n",
      "Mean loss: 0.02435481637949124\n",
      "EPOCH: 3\n",
      "Mean loss: 0.02435491163050756\n",
      "EPOCH: 4\n",
      "Mean loss: 0.024355067731812598\n",
      "EPOCH: 5\n",
      "Mean loss: 0.02435526023618877\n",
      "EPOCH: 6\n",
      "Mean loss: 0.02435509084025398\n",
      "EPOCH: 7\n",
      "Mean loss: 0.024355271260719746\n",
      "EPOCH: 8\n",
      "Mean loss: 0.024355213157832623\n",
      "EPOCH: 9\n",
      "Mean loss: 0.024354795308317988\n",
      "EPOCH: 10\n",
      "Mean loss: 0.024355328641831875\n",
      "EPOCH: 11\n",
      "Mean loss: 0.024354966881219298\n",
      "EPOCH: 12\n",
      "Mean loss: 0.024354904249776155\n",
      "EPOCH: 13\n",
      "Mean loss: 0.024354834528639912\n",
      "EPOCH: 14\n",
      "Mean loss: 0.024354582419618964\n",
      "EPOCH: 15\n",
      "Mean loss: 0.024354537774343044\n",
      "EPOCH: 16\n",
      "Mean loss: 0.02435455977683887\n",
      "EPOCH: 17\n",
      "Mean loss: 0.024354522698558867\n",
      "EPOCH: 18\n",
      "Mean loss: 0.024354453955311328\n",
      "EPOCH: 19\n",
      "Mean loss: 0.02435439688852057\n",
      "EPOCH: 20\n",
      "Mean loss: 0.024354374222457408\n",
      "EPOCH: 21\n",
      "Mean loss: 0.024354357027914376\n",
      "EPOCH: 22\n",
      "Mean loss: 0.024354365258477628\n",
      "EPOCH: 23\n",
      "Mean loss: 0.02435439625987783\n",
      "EPOCH: 24\n",
      "Mean loss: 0.024354394094552846\n",
      "EPOCH: 25\n",
      "Mean loss: 0.024354444851633163\n",
      "EPOCH: 26\n",
      "Mean loss: 0.02435452217468992\n",
      "EPOCH: 27\n",
      "Mean loss: 0.024354532931465656\n",
      "EPOCH: 28\n",
      "Mean loss: 0.024354671977926047\n",
      "EPOCH: 29\n",
      "Mean loss: 0.024354826717171818\n",
      "EPOCH: 30\n",
      "Mean loss: 0.024354675866197793\n",
      "EPOCH: 31\n",
      "Mean loss: 0.024354736029636115\n",
      "EPOCH: 32\n",
      "Mean loss: 0.024354666459839792\n",
      "EPOCH: 33\n",
      "Mean loss: 0.024354471545666456\n",
      "EPOCH: 34\n",
      "Mean loss: 0.024354470032267272\n",
      "EPOCH: 35\n",
      "Mean loss: 0.024354446586221455\n",
      "EPOCH: 36\n",
      "Mean loss: 0.02435445609735325\n",
      "EPOCH: 37\n",
      "Mean loss: 0.024354507552925498\n",
      "EPOCH: 38\n",
      "Mean loss: 0.02435450069606304\n",
      "EPOCH: 39\n",
      "Mean loss: 0.024354509764816613\n",
      "EPOCH: 40\n",
      "Mean loss: 0.024354462104383856\n",
      "EPOCH: 41\n",
      "Mean loss: 0.02435445737792179\n",
      "EPOCH: 42\n",
      "Mean loss: 0.02435446735471487\n",
      "EPOCH: 43\n",
      "Mean loss: 0.024354537867475302\n",
      "EPOCH: 44\n",
      "Mean loss: 0.024354606226552278\n",
      "EPOCH: 45\n",
      "Mean loss: 0.024354682955890893\n",
      "EPOCH: 46\n",
      "Mean loss: 0.024354685202706606\n",
      "EPOCH: 47\n",
      "Mean loss: 0.024354687554296105\n",
      "EPOCH: 48\n",
      "Mean loss: 0.024354752444196492\n",
      "EPOCH: 49\n",
      "Mean loss: 0.024354682362172753\n",
      "EPOCH: 50\n",
      "Mean loss: 0.024354625144042075\n",
      "EPOCH: 51\n",
      "Mean loss: 0.024354540987405925\n",
      "EPOCH: 52\n",
      "Mean loss: 0.024354673409834503\n",
      "EPOCH: 53\n",
      "Mean loss: 0.024354640778619797\n",
      "EPOCH: 54\n",
      "Mean loss: 0.02435478908009827\n",
      "EPOCH: 55\n",
      "Mean loss: 0.024355136207304895\n",
      "EPOCH: 56\n",
      "Mean loss: 0.024354955193120986\n",
      "EPOCH: 57\n",
      "Mean loss: 0.024354591360315682\n",
      "EPOCH: 58\n",
      "Mean loss: 0.024354404129553586\n",
      "EPOCH: 59\n",
      "Mean loss: 0.024354325770400465\n",
      "EPOCH: 60\n",
      "Mean loss: 0.024354282044805588\n",
      "EPOCH: 61\n",
      "Mean loss: 0.02435426552547142\n",
      "EPOCH: 62\n",
      "Mean loss: 0.024354229343589395\n",
      "EPOCH: 63\n",
      "Mean loss: 0.02435422776034102\n",
      "EPOCH: 64\n",
      "Mean loss: 0.024354220612440258\n",
      "EPOCH: 65\n",
      "Mean loss: 0.024354232929181308\n",
      "EPOCH: 66\n",
      "Mean loss: 0.024354265339206906\n",
      "EPOCH: 67\n",
      "Mean loss: 0.02435431513004005\n",
      "EPOCH: 68\n",
      "Mean loss: 0.024354370369110256\n",
      "EPOCH: 69\n",
      "Mean loss: 0.024354453722480685\n",
      "EPOCH: 70\n",
      "Mean loss: 0.024354467284865677\n",
      "EPOCH: 71\n",
      "Mean loss: 0.024354456167202443\n",
      "EPOCH: 72\n",
      "Mean loss: 0.024354387947823852\n",
      "EPOCH: 73\n",
      "Mean loss: 0.024354366841726004\n",
      "EPOCH: 74\n",
      "Mean loss: 0.024354224652051927\n",
      "EPOCH: 75\n",
      "Mean loss: 0.02435419716639444\n",
      "EPOCH: 76\n",
      "Mean loss: 0.02435427795862779\n",
      "EPOCH: 77\n",
      "Mean loss: 0.024354346143081785\n",
      "EPOCH: 78\n",
      "Mean loss: 0.02435448531759903\n",
      "EPOCH: 79\n",
      "Mean loss: 0.024354472558479758\n",
      "EPOCH: 80\n",
      "Mean loss: 0.024354376923292877\n",
      "EPOCH: 81\n",
      "Mean loss: 0.024354291870258747\n",
      "EPOCH: 82\n",
      "Mean loss: 0.024354222521651536\n",
      "EPOCH: 83\n",
      "Mean loss: 0.024354233883786946\n",
      "EPOCH: 84\n",
      "Mean loss: 0.02435431188205257\n",
      "EPOCH: 85\n",
      "Mean loss: 0.02435430441983044\n",
      "EPOCH: 86\n",
      "Mean loss: 0.02435444693546742\n",
      "EPOCH: 87\n",
      "Mean loss: 0.024354636145289987\n",
      "EPOCH: 88\n",
      "Mean loss: 0.024354664608836174\n",
      "EPOCH: 89\n",
      "Mean loss: 0.02435462693683803\n",
      "EPOCH: 90\n",
      "Mean loss: 0.024354934773873536\n",
      "EPOCH: 91\n",
      "Mean loss: 0.02435481594875455\n",
      "EPOCH: 92\n",
      "Mean loss: 0.024354422686155886\n",
      "EPOCH: 93\n",
      "Mean loss: 0.02435435838997364\n",
      "EPOCH: 94\n",
      "Mean loss: 0.024354188749566675\n",
      "EPOCH: 95\n",
      "Mean loss: 0.024354198214132337\n",
      "EPOCH: 96\n",
      "Mean loss: 0.02435418061213568\n",
      "EPOCH: 97\n",
      "Mean loss: 0.02435423230053857\n",
      "EPOCH: 98\n",
      "Mean loss: 0.02435422835405916\n",
      "EPOCH: 99\n",
      "Mean loss: 0.02435431325575337\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  # Just train on the first 5 examples\n",
    "  for k in keys[:5]:\n",
    "    input_image = transformed_data[k]['image'].to(device)\n",
    "    input_size = transformed_data[k]['input_size']\n",
    "    original_image_size = transformed_data[k]['original_image_size']\n",
    "    \n",
    "    # No grad here as we don't want to optimise the encoders\n",
    "    with torch.no_grad():\n",
    "      image_embedding = sam_model.image_encoder(input_image)\n",
    "      \n",
    "      prompt_box = bbox_coords[k]\n",
    "      box = transform.apply_boxes(prompt_box, original_image_size)\n",
    "      box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "      box_torch = box_torch[None, :]\n",
    "      \n",
    "      sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "          points=None,\n",
    "          boxes=box_torch,\n",
    "          masks=None,\n",
    "      )\n",
    "      \n",
    "    \n",
    "    low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "    #binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "    binary_mask = torch.norm(upscaled_masks)\n",
    "    binary_mask = upscaled_masks.div(binary_mask.expand_as(upscaled_masks))\n",
    "\n",
    "    gt_mask_resized = torch.from_numpy(np.resize(ground_truth_mask[k], (1, 1, ground_truth_mask[k].shape[0], ground_truth_mask[k].shape[1]))).to(device)\n",
    "    gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "    \n",
    "    # binary_mask = Variable(torch.where(binary_mask != 0, torch.tensor(1, dtype=torch.float, requires_grad=True), binary_mask), requires_grad=True)\n",
    "    # gt_binary_mask = torch.where(gt_binary_mask != 0, torch.tensor(1, dtype=torch.float, requires_grad=True), gt_binary_mask)\n",
    "    \n",
    "    #loss = Variable(loss_fn1(binary_mask, gt_binary_mask), requires_grad=True)\n",
    "    loss = loss_fn1(binary_mask, gt_binary_mask)\n",
    "    \n",
    "    loss.retain_grad()\n",
    "    binary_mask.retain_grad()\n",
    "    upscaled_masks.retain_grad()\n",
    "    low_res_masks.retain_grad()\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # print(loss)\n",
    "    '''\n",
    "    print(\"________________________\")\n",
    "    print(loss)\n",
    "    print(\"loss grad\", loss.grad, loss.is_leaf)\n",
    "    print(\"binary_mask grad\", binary_mask.grad.sum(), binary_mask.is_leaf)\n",
    "    print(\"upscaled masks grad\", upscaled_masks.grad.sum(), upscaled_masks.is_leaf)\n",
    "    print(\"low_res_masks grad\", low_res_masks.grad.sum(), low_res_masks.is_leaf)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    epoch_losses.append(loss.item())\n",
    "  losses.append(epoch_losses)\n",
    "  print(f'EPOCH: {epoch}')\n",
    "  print(f'Mean loss: {mean(epoch_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': sam_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    }, f'./checkpoints/task2_{epoch}.pth')\n",
    "\n",
    "# Load the checkpoint\n",
    "# checkpoint = torch.load('./checkpoints/task2_.pth')\n",
    "# sam_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
